





# Imports (only what we use)
import random
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from typing import Tuple, List



# Reproducibility
SEED = 123
np.random.seed(SEED)
random.seed(SEED)


# Plot defaults
plt.rcParams.update({
"figure.dpi": 120,
"axes.labelsize": 11,
"axes.titlesize": 12,
"xtick.labelsize": 10,
"ytick.labelsize": 10,
})


# Tidy, labeled printing
from typing import Any


def p(label: str, value: Any):
    print(f"{label}: {value}")


# Helper: average shortest path on largest connected component
import math


def avg_shortest_path_len_lcc(G: nx.Graph) -> float:
    if nx.is_connected(G):
        return nx.average_shortest_path_length(G)
    lcc = G.subgraph(max(nx.connected_components(G), key=len)).copy()
    return nx.average_shortest_path_length(lcc)




















n, k = 500, 4
p_list = [0.0, 0.1, 1.0]


# Use a fixed Random Generator to pass plain ints as seeds to NetworkX
rng = np.random.default_rng(SEED)

# Captions for each value of p
captions = {
    0: "p=0: Regular lattice – high clustering, very long paths.",
    0.1: "p=0.1: Small-world regime – shortcuts shorten paths, clustering remains.",
    1: "p=1: Random graph – minimal path length, almost no clustering."
}

results = []
for p_val in p_list:
    # Different seeds per instance for fairness, but reproducible overall
    seed_val = int(rng.integers(1_000_000_000))
    G = nx.watts_strogatz_graph(n=n, k=k, p=p_val, seed=seed_val)
    d = avg_shortest_path_len_lcc(G)
    results.append((p_val, d))
    # Plot the graph
    plt.figure()
    nx.draw(G, node_size=10)
    plt.title('Watts-Strogatz graph with p = ' + str(p_val))
    if p_val in captions:
        plt.figtext(0.5, -0.05, captions[p_val], wrap=True,
                    ha='center', fontsize=9)

    plt.show()


for prob, d_val in results:
    print(f"WS ⟨d⟩ (n={n}, k={k}, p={prob}): {d_val:.3f}")














n, k = 500, 4
p_values = [0.0, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.4, 1.0]
num_trials = 20


rng = np.random.default_rng(SEED)


def estimate_path_length(n, k, p, trials, rng):
    vals = []
    for _ in range(trials):
        seed_val = int(rng.integers(1_000_000_000))
        G = nx.watts_strogatz_graph(n=n, k=k, p=p, seed=seed_val)
        vals.append(avg_shortest_path_len_lcc(G))
    return float(np.mean(vals)), float(np.std(vals))


means, stds = [], []
for p in p_values:
    m, s = estimate_path_length(n, k, p, num_trials, rng)
    means.append(m)
    stds.append(s)


# Find threshold p* where mean is within 10% of the p=1 mean
m1 = means[-1]
threshold = 1.10 * m1
p_star = None
for p, m in zip(p_values, means):
    if m <= threshold:
        p_star = p
        break


print("⟨d⟩ at p=1 (baseline)", f"{m1:.3f}")
print("10% threshold", f"≤ {threshold:.3f}")
print("Smallest p with ⟨d⟩ within 10% of p=1", p_star)








n, k = 500, 4
p_values = [0.0, 0.01, 0.03, 0.05, 0.1, 0.2, 1.0]
num_trials = 50


rng = np.random.default_rng(SEED)
means, stds = [], []
for p in p_values:
    d_list = []
    for _ in range(num_trials):
        seed_val = int(rng.integers(1_000_000_000))
        G = nx.watts_strogatz_graph(n=n, k=k, p=p, seed=seed_val)
        d_list.append(avg_shortest_path_len_lcc(G))
    means.append(float(np.mean(d_list)))
    stds.append(float(np.std(d_list)))


# Plot
plt.figure(figsize=(8, 6))
plt.errorbar(p_values, means, yerr=stds, fmt="o-", capsize=5)
plt.xlabel("Rewiring probability p")
plt.ylabel("Average shortest path length ⟨d⟩")
plt.title(f"Watts–Strogatz: ⟨d⟩ vs p (n={n}, k={k}, trials={num_trials})")
plt.grid(True, alpha=0.4)

# Add caption below the plot
caption = ("The figure shows the average shortest path length ⟨d⟩ as a function of the rewiring probability p in the "
          "Watts–Strogatz model with fixed n = 500 and k = 4. Error bars denote the standard deviation across 50 realizations. "
          "⟨d⟩ drops steeply between p ≈ 0.01–0.1, reaching values close to the fully randomized case by p ≈ 0.1. "
          "This quantifies the small-world effect: tiny randomness yields near-random path lengths while retaining "
          "substantial clustering for intermediate p (qualitatively expected in WS).")
plt.figtext(0.1, -0.2, caption, wrap=True, horizontalalignment='left', fontsize=8)

plt.tight_layout()
plt.show()




















# Pretty printer for labeled outputs
def p(label, value):
    print(f"{label}: {value}")
# Start from two nodes connected by one edge
G = nx.Graph()
G.add_edge(0, 1)


p("initial |V|, |E|", f"{G.number_of_nodes()}, {G.number_of_edges()}")








def grow_ba_m1(G: nx.Graph, target_n: int, rng: np.random.Generator) -> nx.Graph:
    """Grow the graph to target_n nodes via preferential attachment with m=1.
    Efficiently samples endpoints using a degree-proportional pool.
    """
    # Build initial pool with node IDs repeated by their degree
    pool: List[int] = []
    for u, deg in G.degree():
        pool.extend([u] * deg)


    next_node = max(G.nodes) + 1 if G.number_of_nodes() > 0 else 0


    while G.number_of_nodes() < target_n:
        # Sample an existing node with probability proportional to degree
        # (uniform over the pool equals degree-proportional over nodes)
        attach_to = int(pool[rng.integers(len(pool))])


        # Add the new node and the edge (no self-loop possible since new node is fresh)
        v = next_node
        G.add_edge(v, attach_to)


        # Update pool: +1 copy for attach_to, +1 for the new node (degree 1)
        pool.append(attach_to)
        pool.append(v)


        next_node += 1
    return G








G100 = grow_ba_m1(G.copy(), 100, rng)


# Fixed layout for reproducibility
pos = nx.spring_layout(G100, seed=SEED)
plt.figure()
nx.draw_networkx(G100, pos=pos, with_labels=False, node_size=20)
plt.title("BA model (m=1): 100 nodes")
plt.axis("off")
plt.tight_layout()
plt.show()








G5000 = grow_ba_m1(G.copy(), 5000, rng)


# Degree vector and basic stats
deg = np.array([d for _, d in G5000.degree()])
min_k, max_k = int(deg.min()), int(deg.max())
mean_k = float(deg.mean())


p("BA (m=1) |V|, |E|", f"{G5000.number_of_nodes()}, {G5000.number_of_edges()}")
p("min degree k_min", min_k)
p("max degree k_max", max_k)
p("average degree ⟨k⟩", f"{mean_k:.3f}")








# Histogram bins (auto); return counts and bin edges
counts, bin_edges = np.histogram(deg, bins="auto")
centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])


# Linear scale
plt.figure()
plt.plot(centers, counts, marker="o")
plt.xlabel("Degree k")
plt.ylabel("Count")
plt.title("BA (m=1): Degree distribution — linear scale")
plt.tight_layout()
plt.show()


# Log–log scale
plt.figure()
# Add +1 to avoid log(0) for empty bins; omit zeros by masking instead
mask = counts > 0
plt.loglog(centers[mask], counts[mask], marker="o")
plt.xlabel("Degree k (log)")
plt.ylabel("Count (log)")
plt.title("BA (m=1): Degree distribution — log–log scale")
plt.tight_layout()
plt.show()














import networkx as nx
import os
import math
import random
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import pickle

# Reproducibility
random.seed(42)
np.random.seed(42)


# Matplotlib defaults (no custom colors)
plt.rcParams["figure.dpi"] = 120
plt.rcParams["savefig.dpi"] = 120

# Load the LWCC graph using Python's pickle, as requested
graph_path = "rock_performers_LWCC.gpickle"
assert os.path.exists(graph_path), f"Missing {graph_path}. Re-open Week 4 and run the 'Save for Week 5' cell."

with open(graph_path, "rb") as f:
    H = pickle.load(f)

print(f"Loaded H (Week 4): {H.number_of_nodes()} nodes, {H.number_of_edges()} edges, directed={H.is_directed()}")






N = H.number_of_nodes()
M = H.number_of_edges()

# Basic stats
is_directed = H.is_directed()
density = nx.density(H)

# Components (directed)
wcc_sizes = sorted((len(c) for c in nx.weakly_connected_components(H)), reverse=True)
scc_sizes = sorted((len(c) for c in nx.strongly_connected_components(H)), reverse=True)

# Degree stats
in_degs = dict(H.in_degree()) if is_directed else dict(H.degree())
out_degs = dict(H.out_degree()) if is_directed else dict(H.degree())

avg_in = float(np.mean(list(in_degs.values()))) if is_directed else float(np.mean(list(in_degs.values())))
avg_out = float(np.mean(list(out_degs.values()))) if is_directed else float(np.mean(list(out_degs.values())))

print(
    f"N={N} nodes | M={M} directed edges\n"
    f"density={density:.6f}\n"
    f"weakly connected components: {len(wcc_sizes)} (largest={wcc_sizes[0] if wcc_sizes else 0})\n"
    f"strongly connected components: {len(scc_sizes)} (largest={scc_sizes[0] if scc_sizes else 0})\n"
    f"avg in-degree={avg_in:.3f} | avg out-degree={avg_out:.3f}"
)









# In-degree
fig = plt.figure()
plt.hist(list(in_degs.values()), bins=30)
plt.title("In-degree distribution (LWCC)")
plt.xlabel("In-degree")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# Out-degree
fig = plt.figure()
plt.hist(list(out_degs.values()), bins=30)
plt.title("Out-degree distribution (LWCC)")
plt.xlabel("Out-degree")
plt.ylabel("Count")
plt.tight_layout()
plt.show()









# Random network (same N, M)
G_rand = nx.gnm_random_graph(N, M, directed=True)
rand_out = [d for _, d in G_rand.out_degree()]

fig = plt.figure()
plt.hist(list(out_degs.values()), bins=30, alpha=0.6, label="Rock network")
plt.hist(rand_out, bins=30, alpha=0.6, label="Random (GNM)")
plt.title("Out-degree: rock vs random")
plt.xlabel("Out-degree")
plt.ylabel("Count")
plt.legend()
plt.tight_layout()
plt.show()

# Scale-free-ish reference for in-degree (NetworkX scale_free_graph)
G_sf = nx.scale_free_graph(N)
sf_in = [d for _, d in G_sf.in_degree()]

fig = plt.figure()
plt.hist(list(in_degs.values()), bins=30, alpha=0.6, label="Rock network")
plt.hist(sf_in, bins=30, alpha=0.6, label="Scale-free model")
plt.title("In-degree: rock vs scale-free")
plt.xlabel("In-degree")
plt.ylabel("Count")
plt.legend()
plt.tight_layout()
plt.show()





# Assemble a tidy table
content_len = nx.get_node_attributes(H, "content_length")

rows = []
for n in H.nodes():
    rows.append({
        "artist": n,
        "in_degree": in_degs.get(n, 0),
        "out_degree": out_degs.get(n, 0),
        "content_length": content_len.get(n, np.nan)
    })

df = pd.DataFrame(rows)

# Top 5 by in-degree / out-degree
top_in = df.nlargest(5, "in_degree")["artist"].tolist()
top_out = df.nlargest(5, "out_degree")["artist"].tolist()

print("Top 5 by in-degree:")
print(df.nlargest(5, "in_degree")[ ["artist", "in_degree"] ].to_string(index=False))
print("\nTop 5 by out-degree:")
print(df.nlargest(5, "out_degree")[ ["artist", "out_degree"] ].to_string(index=False))

# Top 10 by content length
print("\nTop 10 by content length:")
print(df.nlargest(10, "content_length")[ ["artist", "content_length"] ].to_string(index=False))









# Select a manageable subgraph for plotting
keep = set(df.nlargest(250, "in_degree")["artist"]) if N > 300 else set(H.nodes())
H_plot = H.subgraph(keep).copy()

# Force-directed layout (deterministic via seed)
pos = nx.spring_layout(H_plot, seed=42)

# Node sizes by in-degree (scaled)
sizes = np.array([in_degs.get(n, 0) for n in H_plot.nodes()])
node_sizes = 60 * (1 + sizes / (sizes.max() if sizes.max() > 0 else 1))

fig = plt.figure(figsize=(7.2, 7.2))
plt.axis('off')
nx.draw_networkx_edges(H_plot, pos, alpha=0.3)
nx.draw_networkx_nodes(H_plot, pos, node_size=node_sizes)
# Label a few hubs (top 10 by in-degree)
for n in df.nlargest(10, "in_degree")["artist"]:
    if n in H_plot:
        x, y = pos[n]
        plt.text(x, y, n.replace('_', ' '), fontsize=7)
plt.title("Rock network — top nodes by in-degree (subgraph)")
plt.tight_layout()
plt.show()








from matplotlib.colors import Normalize
from matplotlib.cm import ScalarMappable

# 1) Make an undirected view to visualize (keep H for later!)
G_und = H.to_undirected()

print(f"Undirected for viz: {G_und.number_of_nodes()} nodes, {G_und.number_of_edges()} edges")

# 2) Node metrics for viz
deg_dict = dict(G_und.degree())
degrees = np.array([deg_dict[n] for n in G_und.nodes()])

# Content length (fallback to 0 if missing)
content = np.array([H.nodes[n].get("content_length", 0) for n in G_und.nodes()])

# 3) Scale node sizes by degree (tweak multipliers to taste)
#    Use sqrt to compress very high degrees so sizes stay reasonable.
node_sizes = 80 + 15 * np.sqrt(degrees)

# 4) Map content length to color (Sequential colormap)
cmap = plt.cm.viridis  # simple sequential map
norm = Normalize(vmin=content.min(), vmax=content.max(), clip=True)
node_colors = cmap(norm(content))

# 5) Two-stage layout to reduce bad local minima:
#    Start with Kamada–Kawai, then refine with Spring using KK as init.
print("Computing layouts (KK init -> Spring refine)...")
pos= nx.kamada_kawai_layout(G_und, weight=None)   # ignore weights; positions only

# 6) Draw
fig, ax = plt.subplots(figsize=(12, 10))
nx.draw_networkx_edges(G_und, pos, width=0.4, alpha=0.25, edge_color="#777777", ax=ax)
nx.draw_networkx_nodes(
    G_und, pos,
    node_size=node_sizes,
    node_color=node_colors,
    linewidths=0.3,
    edgecolors="black",
    ax=ax
)
#label a handful of high-degree nodes to avoid clutter
label_nodes = set(sorted(deg_dict, key=deg_dict.get, reverse=True)[:12])
labels = {n: n.replace("_", " ") for n in G_und.nodes() if n in label_nodes}
nx.draw_networkx_labels(G_und, pos, labels=labels, font_size=8, ax=ax)

# 7) Colorbar for content length
sm = ScalarMappable(norm=norm, cmap=cmap)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax, fraction=0.035, pad=0.02)
cbar.set_label("Wiki content length (words)")

plt.title("Rock Performer Network (Undirected view)\nSize = degree, Color = content length", pad=12)
plt.axis("off")
plt.tight_layout()
plt.show()









# 1) Make an undirected view to visualize (keep H for later!)
G_und = H.to_undirected()

print(f"Undirected for viz: {G_und.number_of_nodes()} nodes, {G_und.number_of_edges()} edges")

# 2) Node metrics for viz
deg_dict = dict(G_und.degree())
degrees = np.array([deg_dict[n] for n in G_und.nodes()])

# Content length (fallback to 0 if missing)
content = np.array([H.nodes[n].get("content_length", 0) for n in G_und.nodes()])

# 3) Scale node sizes by degree (tweak multipliers to taste)
#    Use sqrt to compress very high degrees so sizes stay reasonable.
node_sizes = 80 + 15 * np.sqrt(degrees)

# 4) Map content length to color (Sequential colormap)
cmap = plt.cm.viridis  # simple sequential map
norm = Normalize(vmin=content.min(), vmax=content.max(), clip=True)
node_colors = cmap(norm(content))

# 5) Two-stage layout to reduce bad local minima:
#    Start with Kamada–Kawai, then refine with Spring using KK as init.
print("Computing layouts (KK init -> Spring refine)...")
pos = nx.spring_layout(G_und, pos=pos, iterations=150, seed=42)

# 6) Draw
fig, ax = plt.subplots(figsize=(12, 10))
nx.draw_networkx_edges(G_und, pos, width=0.4, alpha=0.25, edge_color="#777777", ax=ax)
nx.draw_networkx_nodes(
    G_und, pos,
    node_size=node_sizes,
    node_color=node_colors,
    linewidths=0.3,
    edgecolors="black",
    ax=ax
)
# label a handful of high-degree nodes to avoid clutter
label_nodes = set(sorted(deg_dict, key=deg_dict.get, reverse=True)[:12])
labels = {n: n.replace("_", " ") for n in G_und.nodes() if n in label_nodes}
nx.draw_networkx_labels(G_und, pos, labels=labels, font_size=8, ax=ax)

# 7) Colorbar for content length
sm = ScalarMappable(norm=norm, cmap=cmap)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax, fraction=0.035, pad=0.02)
cbar.set_label("Wiki content length (words)")

plt.title("Rock Performer Network (Undirected view)\nSize = degree, Color = content length", pad=12)
plt.axis("off")
plt.tight_layout()
plt.show()













